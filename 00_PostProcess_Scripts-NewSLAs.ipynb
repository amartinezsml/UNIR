{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fb18e-3a86-4e03-aaff-2b11a83cc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetimez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de95df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Functions\n",
    "def files_s3_bucket(Bucketname, Folder, Extension):\n",
    "   \"\"\"\n",
    "   Returns the list of files in an S3 bucket filtered by folder and file extension.\n",
    "   Args:\n",
    "       Bucketname (str): Name of the S3 bucket, e.g. 'voiceofvehicle'\n",
    "       Folder (str): Prefix/folder inside the bucket, e.g. '0_TEST/'\n",
    "       Extension (str): File extension to filter by, e.g. '.csv'\n",
    "   Returns:\n",
    "       list[str]: List of object keys (paths) matching the folder and extension.\n",
    "   \"\"\"\n",
    "   s3 = boto3.client('s3')\n",
    "   response = s3.list_objects_v2(Bucket=Bucketname, Prefix=Folder)\n",
    "\n",
    "\n",
    "   object_keys = [\n",
    "       obj['Key'] for obj in response.get('Contents', [])\n",
    "       if obj['Key'] != Folder and obj['Key'].endswith(Extension)\n",
    "   ]\n",
    "   return object_keys\n",
    "\n",
    "def list_all_s3_folders(bucket_name, prefix=''):\n",
    "   \"\"\"\n",
    "   Recursively list all folders (prefixes) in an S3 bucket.\n",
    "   Args:\n",
    "       bucket_name (str): The name of the S3 bucket.\n",
    "       prefix (str): Optional base path to start from (e.g. 'data/')\n",
    "   Returns:\n",
    "       list[str]: A sorted list of unique folder paths.\n",
    "   \"\"\"\n",
    "   s3 = boto3.client('s3')\n",
    "   paginator = s3.get_paginator('list_objects_v2')\n",
    "   # Store found folder paths\n",
    "   folders = set()\n",
    "   for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "       for obj in page.get('Contents', []):\n",
    "           key = obj['Key']\n",
    "           if '/' in key:\n",
    "               parts = key.split('/')[:-1]  # drop filename, keep folders\n",
    "               for i in range(1, len(parts) + 1):\n",
    "                   folders.add('/'.join(parts[:i]) + '/')\n",
    "   return sorted(folders)\n",
    "\n",
    "def preserve_acl_before_upload(bucket, key):\n",
    "   import boto3\n",
    "   s3 = boto3.client('s3')\n",
    "   try:\n",
    "       return s3.get_object_acl(Bucket=bucket, Key=key)\n",
    "   except ClientError as e:\n",
    "       if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "           return None\n",
    "       else:\n",
    "           raise\n",
    "def preserve_acl_before_upload2(s3, bucket, key):\n",
    "   try:\n",
    "       return s3.get_object_acl(Bucket=bucket, Key=key)\n",
    "   except ClientError as e:\n",
    "       if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "           return None\n",
    "       else:\n",
    "           raise\n",
    "\n",
    "def reapply_acl_after_upload(bucket, key, acl):\n",
    "   import boto3\n",
    "   s3 = boto3.client('s3')\n",
    "   if acl:\n",
    "       s3.put_object_acl(\n",
    "           Bucket=bucket,\n",
    "           Key=key,\n",
    "           AccessControlPolicy={\n",
    "               'Grants': acl['Grants'],\n",
    "               'Owner': acl['Owner']\n",
    "           }\n",
    "       )\n",
    "def reapply_acl_after_upload2(s3, bucket, key, acl):\n",
    "   if acl:\n",
    "       s3.put_object_acl(Bucket=bucket, Key=key, AccessControlPolicy=acl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9588199-4dee-4b91-98bf-b22063f57260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path containing .xls files\n",
    "folder_path =\"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Raw_Data/\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".xls\"):# Check if the file is an .xls file\n",
    "        file_path = os.path.join(folder_path, file_name) # Construct the full file path\n",
    "        df = pd.read_excel(file_path)# Read the Excel file into a DataFrame\n",
    "        dfs.append(df)# Append the DataFrame to the lis\n",
    "        week_number = pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').week\n",
    "        month=pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').month\n",
    "        month=pd.to_datetime(month, format='%m').strftime('%B')\n",
    "        date=pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').date()\n",
    "        date_datetime=pd.to_datetime(date)\n",
    "    \n",
    "merged_df = pd.concat(dfs, ignore_index=True)# Merge all DataFrames in the list into a single DataFrame\n",
    "\n",
    "# Define the renaming mapping\n",
    "rename_map = {\n",
    "    \"Comment\": \"0 Comment\",\n",
    "    \"Problem Statement - Anecdote\": \"1 Problem Statement - Anecdote\",\n",
    "    \"Immediate Action\": \"2 Immediate Action\",\n",
    "    \"Problem Definition and Severity\": \"3 Problem Definition and Severity\",\n",
    "    \"Short Term Fix\": \"4 Short Term Fix\",\n",
    "    \"Root Cause\": \"5 Root Cause\",\n",
    "    \"Long term fix technical development and validation\": \"6 Long term fix technical development and validation\",\n",
    "    \"Long term fix deployment\": \"7 Long term fix deployment\",\n",
    "    \"Lessons Learned\": \"8 Lessons Learned\"}\n",
    "\n",
    "# Apply renaming to the \"NextStepAction\" column\n",
    "merged_df['NextStepAction'] = merged_df['NextStepAction'].map(rename_map)\n",
    "\n",
    "# Print the merged DataFrame\n",
    "merged_df.head(4)\n",
    "\n",
    "print(date_datetime)\n",
    "\n",
    "df=merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d967b-0c15-4e17-8a94-cfc6de0199b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'LastUpdatedDate' and 'CreateDate' columns to datetime if they're not already in datetime format\n",
    "df['LastUpdatedDate'] = pd.to_datetime(df['LastUpdatedDate'])\n",
    "df['CreateDate'] = pd.to_datetime(df['CreateDate'])\n",
    "\n",
    "# Sort the dataframe by 'ShortId', 'NextStepAction', and 'LastUpdatedDate'\n",
    "df = df.sort_values(by=['ShortId', 'LastUpdatedDate'])\n",
    "\n",
    "# Initialize a new column to store the duration in the same status as integers (days only)\n",
    "df['TimeInSameStatus'] = 0\n",
    "\n",
    "# Initialize a new column to store the age of the ticket based on CreateDate\n",
    "df['Age'] = (date_datetime - df['CreateDate']).dt.days\n",
    "\n",
    "# Initialize a new column to indicate if a ticket is in its last status\n",
    "df['IsLastStatus'] = False\n",
    "\n",
    "# Group by 'ShortId' and iterate through each group to find the last status\n",
    "for short_id, group in df.groupby('ShortId'):\n",
    "    last_index = group.index[-1]\n",
    "    df.at[last_index, 'IsLastStatus'] = True\n",
    "    \n",
    "# Define a function to determine the fix type based on NextStepAction\n",
    "def determine_fix_type(next_step_action):\n",
    "    if next_step_action in [\"3 Problem Definition and Severity\", \"4 Short Term Fix\", \"5 Root Cause\"]:\n",
    "        return \"InShortTermFix\"\n",
    "    else:\n",
    "        return \"InLongTermFix\"\n",
    "\n",
    "# Initialize the 'FixType' column with default value\n",
    "df['FixType'] = \"\"\n",
    "\n",
    "# Group by 'ShortId' and iterate through each group\n",
    "for short_id, group in df.groupby('ShortId'):\n",
    "    last_status = group['NextStepAction'].iloc[-1]\n",
    "    \n",
    "    # Determine the fix type for the entire group\n",
    "    fix_type = determine_fix_type(last_status)\n",
    "    \n",
    "    # Set the 'FixType' column for all rows in the group\n",
    "    df.loc[df['ShortId'] == short_id, 'FixType'] = fix_type\n",
    "\n",
    "# Assuming df is your pandas DataFrame\n",
    "df = df.sort_values(by=['ShortId', 'LastUpdatedDate'])\n",
    "\n",
    "# Initialize a new column to store the time differences\n",
    "df['TimeInSameStatus'] = None\n",
    "\n",
    "# Group by 'ShortId' and iterate through each group\n",
    "for short_id, group in df.groupby('ShortId'):\n",
    "    # Get the last status of the group\n",
    "    last_status = group['Status'].iloc[-1]\n",
    "    # Check if the last status is 'Resolved'\n",
    "    if last_status == 'Resolved':\n",
    "        # Calculate time difference for the last status to the first one\n",
    "        first_date = group['LastUpdatedDate'].iloc[0]\n",
    "        last_date = group['LastUpdatedDate'].iloc[-1]\n",
    "        last_status_diff = (last_date - first_date).days\n",
    "        # Store time difference in the original DataFrame for the last status\n",
    "        df.loc[df['ShortId'] == short_id, 'TimeInSameStatus'] = last_status_diff\n",
    "        # Mark all tickets in the group as 'Resolved'\n",
    "        df.loc[df['ShortId'] == short_id, 'Status'] = 'Resolved'\n",
    "    else:\n",
    "        # Filter the DataFrame for the current shortId\n",
    "        filtered_df = df[df['ShortId'] == short_id]\n",
    "        # Check if the first status is \"Open\"\n",
    "        if filtered_df['Status'].iloc[0] == 'Open':\n",
    "            list_date = []\n",
    "            list_status = []\n",
    "            for step in filtered_df['NextStepAction'].unique():\n",
    "                filtered_df_status = filtered_df[filtered_df['NextStepAction'] == step]\n",
    "                min_date_status = filtered_df_status['LastUpdatedDate'].min()\n",
    "                list_date.append(pd.to_datetime(min_date_status))  # Convert date string to datetime object\n",
    "                list_status.append(step)\n",
    "            # Calculate time differences between consecutive dates for each status\n",
    "            time_diffs = []\n",
    "            for i in range(1, len(list_date)):\n",
    "                time_diff = (list_date[i] - list_date[i-1]).days\n",
    "                time_diffs.append((list_status[i-1], time_diff))\n",
    "            # Calculate time difference for the last status to today\n",
    "            if list_date:\n",
    "                if len(list_date) == 1:  # Check if there's only one status\n",
    "                    last_status_diff = (date_datetime - list_date[0]).days\n",
    "                    # Store time difference in the original DataFrame for the first status\n",
    "                    df.loc[(df['ShortId'] == short_id) & (df['NextStepAction'] == list_status[0]), 'TimeInSameStatus'] = last_status_diff\n",
    "                else:\n",
    "                    last_status_diff = (date_datetime - list_date[-1]).days\n",
    "                    # Store time difference in the original DataFrame for the last status\n",
    "                    df.loc[(df['ShortId'] == short_id) & (df['NextStepAction'] == list_status[-1]), 'TimeInSameStatus'] = last_status_diff\n",
    "            # Store time differences for consecutive statuses\n",
    "            for status, time_diff in time_diffs:\n",
    "                # Store time difference in the original DataFrame for each status\n",
    "                df.loc[(df['ShortId'] == short_id) & (df['NextStepAction'] == status), 'TimeInSameStatus'] = time_diff\n",
    "        else:\n",
    "            list_date = []\n",
    "            list_status = []\n",
    "            for step in filtered_df['NextStepAction'].unique():\n",
    "                filtered_df_status = filtered_df[filtered_df['NextStepAction'] == step]\n",
    "                min_date_status = filtered_df_status['LastUpdatedDate'].min()\n",
    "                list_date.append(pd.to_datetime(min_date_status))  # Convert date string to datetime object\n",
    "                list_status.append(step)\n",
    "            # Calculate time differences between consecutive dates for each status\n",
    "            time_diffs = []\n",
    "            for i in range(1, len(list_date)):\n",
    "                time_diff = (list_date[i] - list_date[i-1]).days\n",
    "                time_diffs.append((list_status[i-1], time_diff))\n",
    "            # Calculate time difference for the last status to the first one\n",
    "            if list_date:\n",
    "                last_status_diff = (list_date[-1] - list_date[0]).days\n",
    "                # Store time difference in the original DataFrame for the last status\n",
    "                df.loc[(df['ShortId'] == short_id) & (df['NextStepAction'] == list_status[-1]), 'TimeInSameStatus'] = last_status_diff\n",
    "            # Store time differences for consecutive statuses\n",
    "            for status, time_diff in time_diffs:\n",
    "                # Store time difference in the original DataFrame for each status\n",
    "                df.loc[(df['ShortId'] == short_id) & (df['NextStepAction'] == status), 'TimeInSameStatus'] = time_diff\n",
    "\n",
    "# Ensure all time differences are positive\n",
    "df['TimeInSameStatus'] = df['TimeInSameStatus'].fillna(0).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512341ec-6e5c-4643-b96b-7a40e6ac461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file exported successfully to: W:/Shared With Me/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Raw_Data/Merged_Data/Raw_Data_Calculation_week_22.csv\n",
      "CSV file exported successfully to: W:/Shared With Me/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Raw_Data/Merged_Data/Data_Merged_with_Calculations.csv\n"
     ]
    }
   ],
   "source": [
    "#Export data\n",
    "\n",
    "# Define the path where you want to save the CSV file\n",
    "file_path = \"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Raw_Data/Merged_Data/\"\n",
    "\n",
    "# Define the filename with last's week number from folder and the path\n",
    "file_name = f\"{file_path}Raw_Data_Calculation_week_{week_number}.csv\"\n",
    "\n",
    "file_name_2 = f\"{file_path}Data_Merged_with_Calculations.csv\"\n",
    "# Export the pivot table DataFrame to a CSV file at the specified path\n",
    "df.to_csv(file_name, index=False)\n",
    "df.to_csv(file_name_2, index=False)\n",
    "\n",
    "print(f\"CSV file exported successfully to: {file_name}\")\n",
    "print(f\"CSV file exported successfully to: {file_name_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1baebab-8b79-45bd-abf2-80afbde6233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcuations of Running totals\n",
    "\n",
    "# List of values to filter by\n",
    "filter_list = ['3 Problem Definition and Severity', '4 Short Term Fix', '5 Root Cause','6 Long term fix technical development and validation','7 Long term fix deployment', '8 Lessons Learned']\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['NextStepAction'].isin(filter_list)]\n",
    "\n",
    "pivot_table = filtered_df.pivot_table(values='TimeInSameStatus',\n",
    "                             index=['Status','Title','NextStepAction' ],\n",
    "                             aggfunc='max',\n",
    "                             fill_value=0)\n",
    "\n",
    "running_totals = pivot_table.groupby(level='Title').cumsum(axis=0)\n",
    "running_totals=running_totals.assign(**{\"Week_number\":week_number})\n",
    "\n",
    "#Export weekly running\n",
    "#file_path_VCM_Metrics_weekly = f\"C:/Users/{user}/Desktop/Python/Dashboard/Weekly_KPI/VCM_Metrics/\"\n",
    "file_path_VCM_Metrics_weekly = \"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Weekly_KPI/VCM_Metrics/\"\n",
    "file_name_VCM_Metrics_weekly = f\"{file_path_VCM_Metrics_weekly}VCM_Metrics_Wk_{week_number}.csv\"\n",
    "running_totals.to_csv(file_name_VCM_Metrics_weekly, index=True)\n",
    "\n",
    "\n",
    "#Export Actual\n",
    "#file_path_VCM_Metrics_actual = f\"C:/Users/{user}/Desktop/Python/Dashboard/Weekly_KPI/VCM_Metrics/Merged/\"\n",
    "file_path_VCM_Metrics_actual = f\"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Weekly_KPI/VCM_Metrics/Merged/\"\n",
    "\n",
    "file_name_VCM_Metrics_actual = f\"{file_path_VCM_Metrics_actual}VCM_Metrics_Actual.csv\"\n",
    "running_totals.to_csv(file_name_VCM_Metrics_actual, index=True)\n",
    "\n",
    "# Loop through each file in the folder\n",
    "dfs = []\n",
    "\n",
    "for file_name in os.listdir(file_path_VCM_Metrics_weekly):\n",
    "    # Check if the file is an .xls file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(file_path_VCM_Metrics_weekly, file_name)\n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "# Merge all DataFrames in the list into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=False)\n",
    "merged_df.to_csv(f\"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Weekly_KPI/VCM_Metrics/Merged/VCM_Metrics_Merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc428fa-0429-4c54-92bd-9fa1045b3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "#Calculation of Goal Metric\n",
    "df=running_totals.reset_index()\n",
    "\n",
    "filter_list = ['3 Problem Definition and Severity', '4 Short Term Fix', '5 Root Cause']\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['NextStepAction'].isin(filter_list)]\n",
    "\n",
    "\n",
    "filtered_df= pd.pivot_table(filtered_df, index=[\"Title\",\"Status\"],values=\"TimeInSameStatus\", aggfunc='max')\n",
    "filtered_df=filtered_df.reset_index()\n",
    "\n",
    "interval = 90\n",
    "min_interval = 0\n",
    "max_interval = filtered_df['TimeInSameStatus'].max()\n",
    "\n",
    "bins = range(min_interval, max_interval + interval, interval)\n",
    "\n",
    "# Cut the data into intervals\n",
    "filtered_df['TimeInSameStatus'] = pd.cut(filtered_df['TimeInSameStatus'], bins)\n",
    "\n",
    "# Create pivot table to count occurrences of each interval\n",
    "Pivot_Groups = pd.pivot_table(filtered_df, index=[\"Status\",\"TimeInSameStatus\"],values=\"Title\", aggfunc=pd.Series.nunique)\n",
    "\n",
    "# Calculate percentage\n",
    "\n",
    "total_counts = Pivot_Groups.groupby('Status')['Title'].transform('sum')\n",
    "Pivot_Groups['Percentage'] = Pivot_Groups['Title'] / total_counts * 100\n",
    "\n",
    "\n",
    "Pivot_Groups=Pivot_Groups.assign(**{\"Week_number\":week_number})\n",
    "\n",
    "#Export weekly running\n",
    "\n",
    "#file_path_VCM_weekly_Goal = f\"C:/Users/{user}/Desktop/Python/Dashboard/Weekly_KPI/VCM_Metrics/Goal_Metric/\"\n",
    "file_path_VCM_weekly_Goal = \"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Weekly_KPI/VCM_Metrics/Goal_Metric/\"\n",
    "file_name_VCM_weekly_Goal = f\"{file_path_VCM_weekly_Goal}VCM_Goal_Metric_Wk_{week_number}.csv\"\n",
    "\n",
    "#file_path_VCM_weekly_Goal_merged= f\"C:/Users/{user}/Desktop/Python/Dashboard/Weekly_KPI/VCM_Metrics/Goal_Metric/Merged/\"\n",
    "file_path_VCM_weekly_Goal_merged= \"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Weekly_KPI/VCM_Metrics/Goal_Metric/Merged/\"\n",
    "file_name_VCM_weekly_Goal_merged=f\"{file_path_VCM_weekly_Goal_merged}VCM_Goal_Metric_Merged.csv\"\n",
    "Pivot_Groups.to_csv(file_name_VCM_weekly_Goal, index=True)\n",
    "\n",
    "# Loop through each file in the folder\n",
    "dfs = []\n",
    "\n",
    "for file_name in os.listdir(file_path_VCM_weekly_Goal):\n",
    "    # Check if the file is an .xls file\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(file_path_VCM_weekly_Goal, file_name)\n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "\n",
    "# Merge all DataFrames in the list into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=False)\n",
    "merged_df.to_csv(file_name_VCM_weekly_Goal_merged, index=False)\n",
    "\n",
    "print ('Successful' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23612e-fb85-459b-90fa-72d51a5432fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subir archivos csv a S3 mantieniendo ACL:\n",
    "file=\"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/\"\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path = f\"{file}Raw_Data/Merged_Data/Data_Merged_with_Calculations.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/Data_Merged_with_Calculations.csv\"\n",
    ")\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path = f\"{file}Weekly_KPI/VCM_Metrics/Merged/VCM_Metrics_Actual.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/VCM_Metrics_Actual.csv\"\n",
    ")\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path= f\"{file}Weekly_KPI/VCM_Metrics/Goal_Metric/Merged/VCM_Goal_Metric_Merged.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/VCM_Goal_Metric_Merged.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7843b-d391-4102-93f1-d112c0f193bd",
   "metadata": {},
   "source": [
    "For TFE screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a39b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path containing .xls files\n",
    "folder_path = \"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/Raw_Data/\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".xls\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_excel(file_path)\n",
    "        dfs.append(df)\n",
    "        week_number = pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').week\n",
    "        month = pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').month\n",
    "        month = pd.to_datetime(month, format='%m').strftime('%B')\n",
    "        date = pd.to_datetime(file_name.split('.')[0], format='%m_%d_%Y').date()\n",
    "        date_datetime = pd.to_datetime(date)\n",
    "\n",
    "# Merge all DataFrames in the list into a single DataFrame\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Define the renaming mapping for standardizing phase names\n",
    "rename_map = {\n",
    "    \"Comment\": \"0 Comment\",\n",
    "    \"Problem Statement - Anecdote\": \"1 Problem Statement - Anecdote\",\n",
    "    \"Immediate Action\": \"2 Immediate Action\",\n",
    "    \"Problem Definition and Severity\": \"3 Problem Definition and Severity\",\n",
    "    \"Short Term Fix\": \"4 Short Term Fix\",\n",
    "    \"Root Cause\": \"5 Root Cause\",\n",
    "    \"Long term fix technical development and validation\": \"6 Long term fix technical development and validation\",\n",
    "    \"Long term fix deployment\": \"7 Long term fix deployment\",\n",
    "    \"Lessons Learned\": \"8 Lessons Learned\"\n",
    "}\n",
    "\n",
    "# Apply renaming to the \"NextStepAction\" column\n",
    "merged_df['NextStepAction'] = merged_df['NextStepAction'].map(rename_map)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "merged_df['LastUpdatedDate'] = pd.to_datetime(merged_df['LastUpdatedDate'])\n",
    "merged_df['CreateDate'] = pd.to_datetime(merged_df['CreateDate'])\n",
    "\n",
    "# Sort chronologically\n",
    "merged_df = merged_df.sort_values(by=['ShortId', 'LastUpdatedDate'])\n",
    "\n",
    "# Save the final cleaned and unified DataFrame\n",
    "df = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec65e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque 1: Conversión de fechas, ordenación y columnas auxiliares\n",
    "# Convert columns to datetime type\n",
    "df['LastUpdatedDate'] = pd.to_datetime(df['LastUpdatedDate'])\n",
    "df['CreateDate'] = pd.to_datetime(df['CreateDate'])\n",
    "\n",
    "# Ordenar por identificador de ticket y fecha de actualización\n",
    "df = df.sort_values(by=['ShortId', 'LastUpdatedDate'])\n",
    "\n",
    "# Inicializar columnas\n",
    "df['TimeInSameStatus'] = 0\n",
    "df['Age'] = (date_datetime - df['CreateDate']).dt.days\n",
    "df['IsLastStatus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque 2: Marcar último estado por ticket\n",
    "for short_id, group in df.groupby('ShortId'):\n",
    "    last_index = group.index[-1]\n",
    "    df.at[last_index, 'IsLastStatus'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca165772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque 3: Calcular tiempo en la misma fase por ticket\n",
    "\n",
    "df = df.sort_values(by=['ShortId', 'LastUpdatedDate'])\n",
    "df['TimeInSameStatus'] = None\n",
    "\n",
    "for short_id, group in df.groupby('ShortId'):\n",
    "    last_status = group['Status'].iloc[-1]\n",
    "\n",
    "    if last_status == 'Resolved':\n",
    "        first_date = group['LastUpdatedDate'].iloc[0]\n",
    "        last_date = group['LastUpdatedDate'].iloc[-1]\n",
    "        last_status_diff = (last_date - first_date).days\n",
    "        df.loc[df['ShortId'] == short_id, 'TimeInSameStatus'] = last_status_diff\n",
    "        df.loc[df['ShortId'] == short_id, 'Status'] = 'Resolved'\n",
    "    else:\n",
    "        list_date = []\n",
    "        list_status = []\n",
    "        for step in group['NextStepAction'].unique():\n",
    "            filtered_df_status = group[group['NextStepAction'] == step]\n",
    "            min_date_status = filtered_df_status['LastUpdatedDate'].min()\n",
    "            list_date.append(pd.to_datetime(min_date_status))\n",
    "            list_status.append(step)\n",
    "\n",
    "        time_diffs = []\n",
    "        for i in range(1, len(list_date)):\n",
    "            time_diff = (list_date[i] - list_date[i-1]).days\n",
    "            time_diffs.append((list_status[i-1], time_diff))\n",
    "\n",
    "        if list_date:\n",
    "            if len(list_date) == 1:\n",
    "                last_status_diff = (date_datetime - list_date[0]).days\n",
    "                df.loc[(df['ShortId'] == short_id) & \n",
    "                       (df['NextStepAction'] == list_status[0]), \n",
    "                       'TimeInSameStatus'] = last_status_diff\n",
    "            else:\n",
    "                last_status_diff = (date_datetime - list_date[-1]).days\n",
    "                df.loc[(df['ShortId'] == short_id) & \n",
    "                       (df['NextStepAction'] == list_status[-1]), \n",
    "                       'TimeInSameStatus'] = last_status_diff\n",
    "\n",
    "        for status, time_diff in time_diffs:\n",
    "            df.loc[(df['ShortId'] == short_id) & \n",
    "                   (df['NextStepAction'] == status), \n",
    "                   'TimeInSameStatus'] = time_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bloque 4: Asegurar valores positivos y sin nulos\n",
    "df['TimeInSameStatus'] = df['TimeInSameStatus'].fillna(0).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c407de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Funciones para preservar y reaplicar ACLs\n",
    "def preserve_acl_before_upload(bucket, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        return s3.get_object_acl(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "            return None\n",
    "        else:\n",
    "            raise\n",
    "def reapply_acl_after_upload(bucket, key, acl):\n",
    "    s3 = boto3.client('s3')\n",
    "    if acl:\n",
    "        s3.put_object_acl(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            AccessControlPolicy={\n",
    "                'Grants': acl['Grants'],\n",
    "                'Owner': acl['Owner']\n",
    "            }\n",
    "        )\n",
    "def upload_file_to_s3_with_acl(file_path, bucket_name, key):\n",
    "    s3 = boto3.client('s3')\n",
    "    acl = preserve_acl_before_upload(bucket_name, key)\n",
    "    s3.upload_file(file_path, bucket_name, key)\n",
    "    reapply_acl_after_upload(bucket_name, key, acl)\n",
    "    print(f\"Archivo {file_path} subido a s3://{bucket_name}/{key} con ACL preservado.\")\n",
    "\n",
    "\n",
    "# Subir archivos csv a S3 mantieniendo ACL:\n",
    "file=\"W:/Shared With Me/GFP Documents/Teams and Programs/EU_VPE_Team/02_Change_Management/2024/01 - Projects/01 - Field Issues/00 - Tickets/Dashboard Updates/Dashboard/\"\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path = f\"{file}Raw_Data/Merged_Data/Data_Merged_with_Calculations.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/Data_Merged_with_Calculations.csv\"\n",
    ")\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path = f\"{file}Weekly_KPI/VCM_Metrics/Merged/VCM_Metrics_Actual.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/VCM_Metrics_Actual.csv\"\n",
    ")\n",
    "\n",
    "upload_file_to_s3_with_acl(\n",
    "    file_path= f\"{file}Weekly_KPI/VCM_Metrics/Goal_Metric/Merged/VCM_Goal_Metric_Merged.csv\",\n",
    "    bucket_name=\"voiceofvehicle\",\n",
    "    key=\"dashboardvcm/VCM_Goal_Metric_Merged.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
